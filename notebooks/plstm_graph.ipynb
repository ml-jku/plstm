{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "num_nodes = 200  # Number of nodes\n",
        "prob_edge = 0.002  # Probability of edge creation in ER graph\n",
        "extra_edge_prob = prob_edge\n",
        "\n",
        "# Step 1: Create an undirected Erd\u0151s\u2013R\u00e9nyi (ER) graph\n",
        "# G = nx.erdos_renyi_graph(num_nodes, prob_edge)\n",
        "G = nx.random_labeled_tree(num_nodes)\n",
        "\n",
        "# Step 2: Add extra random edges while ensuring the graph remains simple\n",
        "for u in range(num_nodes):\n",
        "    for v in range(u + 1, num_nodes):\n",
        "        if not G.has_edge(u, v) and random.random() < extra_edge_prob:\n",
        "            G.add_edge(u, v)\n",
        "\n",
        "\n",
        "# Step 2: Convert to DAG by directing edges based on node numbering\n",
        "\n",
        "\n",
        "def dagify(G: nx.Graph) -> nx.DiGraph:\n",
        "    DAG = nx.DiGraph()\n",
        "    DAG.add_nodes_from(G.nodes())\n",
        "\n",
        "    for u, v in G.edges():\n",
        "        if u < v:\n",
        "            DAG.add_edge(u, v)\n",
        "        else:\n",
        "            DAG.add_edge(v, u)\n",
        "    return DAG\n",
        "\n",
        "\n",
        "DAG = dagify(G)\n",
        "\n",
        "RevDAG = nx.DiGraph()\n",
        "RevDAG.add_nodes_from(G.nodes())\n",
        "\n",
        "for u, v in G.edges():\n",
        "    if u < v:\n",
        "        RevDAG.add_edge(v, u)\n",
        "    else:\n",
        "        RevDAG.add_edge(u, v)\n",
        "\n",
        "\n",
        "print(DAG)\n",
        "# Step 3: Visualize the DAG using a spring layout\n",
        "plt.figure(figsize=(6, 6))\n",
        "pos = {node: (idx, max(min(random.random(), 1.0), -1.0)) for idx, node in enumerate(DAG.nodes())}\n",
        "pos = nx.spring_layout(DAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(DAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max(dict(nx.degree(DAG)).values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rDAG = nx.DiGraph()\n",
        "rDAG.add_nodes_from({*range(16)})\n",
        "rDAG.add_edges_from(tuple((i, i + 1) for i in range(16) if ((i + 1) % 4 != 0)) + tuple((i, i + 4) for i in range(12)))\n",
        "print(rDAG)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "pos = {node: (idx % 4, idx // 4) for idx, node in enumerate(rDAG.nodes())}\n",
        "pos = nx.spring_layout(rDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(rDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def prune_dag_to_multitree(dag: nx.DiGraph):\n",
        "    assert nx.is_directed_acyclic_graph(dag), \"Input must be a DAG.\"\n",
        "\n",
        "    topo_order = list(nx.topological_sort(dag))\n",
        "    multitree = nx.DiGraph()\n",
        "    multitree.add_nodes_from(dag.nodes)\n",
        "\n",
        "    # For fast access:\n",
        "    # ancestor_map[b] = set(a)  means a -> b has been visited (a is an ancestor of b)\n",
        "    # descendant_map[a] = set(b) means a -> b has been visited (b is a descendant of a)\n",
        "    ancestor_map = defaultdict(set)\n",
        "    descendant_map = defaultdict(set)\n",
        "\n",
        "    for current in topo_order:\n",
        "        predecessors = list(dag.predecessors(current))\n",
        "\n",
        "        # Sort predecessors based on topological order\n",
        "        predecessors.sort(key=topo_order.index)\n",
        "\n",
        "        for pred in predecessors:\n",
        "            prior_ancestors = ancestor_map[pred]\n",
        "            if any(current in descendant_map[ancestor] for ancestor in prior_ancestors):\n",
        "                continue  # Skip edge\n",
        "\n",
        "            # Otherwise, add the edge\n",
        "            multitree.add_edge(pred, current)\n",
        "            ancestor_map[current].add(pred)\n",
        "            descendant_map[pred].add(current)\n",
        "            for ancestor in prior_ancestors:\n",
        "                ancestor_map[current].add(ancestor)\n",
        "                descendant_map[ancestor].add(current)\n",
        "\n",
        "    return multitree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_multitree(multitree: nx.DiGraph) -> bool:\n",
        "    if not nx.is_directed_acyclic_graph(multitree):\n",
        "        return False\n",
        "\n",
        "    for node1 in multitree.nodes:\n",
        "        for node2 in multitree.nodes:\n",
        "            if len(list(nx.all_simple_paths(multitree, node1, node2))) > 1:\n",
        "                return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pDAG = prune_dag_to_multitree(DAG)\n",
        "plt.figure(figsize=(6, 6))\n",
        "print(pDAG)\n",
        "pos = {node: (idx, max(min(random.random(), 1.0), -1.0)) for idx, node in enumerate(pDAG.nodes())}\n",
        "pos = nx.spring_layout(pDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(pDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prDAG = prune_dag_to_multitree(rDAG)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "pos = {node: (idx % 4, idx // 4) for idx, node in enumerate(prDAG.nodes())}\n",
        "pos = nx.spring_layout(prDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(prDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_line_graph(dag: nx.DiGraph):\n",
        "    linegraph = nx.DiGraph()\n",
        "    linegraph.add_nodes_from(dag.edges)\n",
        "    for n in dag.nodes:\n",
        "        linegraph.add_edges_from(((p, n), (n, s)) for p in dag.predecessors(n) for s in dag.successors(n))\n",
        "    return linegraph\n",
        "\n",
        "\n",
        "linegraph = create_line_graph(DAG)\n",
        "linegraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lDAG = linegraph\n",
        "plt.figure(figsize=(6, 6))\n",
        "print(lDAG)\n",
        "pos = {node: (idx, max(min(random.random(), 1.0), -1.0)) for idx, node in enumerate(lDAG.nodes())}\n",
        "pos = nx.spring_layout(lDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(lDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linegraph = create_line_graph(DAG)\n",
        "plDAG = prune_dag_to_multitree(linegraph)\n",
        "plt.figure(figsize=(6, 6))\n",
        "print(plDAG)\n",
        "pos = {node: (idx, max(min(random.random(), 1.0), -1.0)) for idx, node in enumerate(plDAG.nodes())}\n",
        "pos = nx.spring_layout(plDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(plDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linegraph = create_line_graph(rDAG)\n",
        "plrDAG = prune_dag_to_multitree(linegraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linegraph = create_line_graph(rDAG)\n",
        "plrDAG = prune_dag_to_multitree(linegraph)\n",
        "plt.figure(figsize=(6, 6))\n",
        "print(plrDAG)\n",
        "pos = {\n",
        "    node: ((node[0] % 4 + node[1] % 4) / 2, (node[0] // 4 + node[1] // 4) / 2)\n",
        "    for idx, node in enumerate(plrDAG.nodes())\n",
        "}\n",
        "pos = nx.spring_layout(plrDAG, iterations=0, pos=pos)  # Spring layout for better visualization\n",
        "nx.draw(plrDAG, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", arrows=True)\n",
        "plt.title(\"Directed Acyclic Graph (DAG) from ER Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "check_multitree(plDAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pLSTM Layer construction in torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DAG.edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DAG.nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_nodes = len(DAG.nodes)\n",
        "num_edges = len(DAG.edges)\n",
        "max_edges = max(\n",
        "    max((d for n, d in DAG.in_degree())), max((d for n, d in DAG.out_degree()))\n",
        ")  # limit to both the in-coming and out-going edges\n",
        "max_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adjacency_backward_array = -torch.ones([num_nodes, max_edges])\n",
        "adjacency_forward_array = -torch.ones([num_nodes, max_edges])\n",
        "incoming_edge_nums = torch.zeros([num_nodes])\n",
        "outgoing_edge_nums = torch.zeros([num_nodes])\n",
        "\n",
        "adjacency_forward_edgemap = {}\n",
        "adjacency_backward_edgemap = {}\n",
        "\n",
        "\n",
        "for node in DAG.nodes:\n",
        "    pred = list(DAG.predecessors(node))\n",
        "    adjacency_backward_array[node, : len(pred)] = torch.tensor(pred)\n",
        "    for idx, pn in enumerate(pred):\n",
        "        adjacency_backward_edgemap[(pn, node)] = idx\n",
        "    incoming_edge_nums[node] = len(pred)\n",
        "\n",
        "for node in DAG.nodes:\n",
        "    succ = list(DAG.successors(node))\n",
        "    adjacency_forward_array[node, : len(succ)] = torch.tensor(succ)\n",
        "    for idx, sn in enumerate(succ):\n",
        "        adjacency_forward_edgemap[(node, sn)] = idx\n",
        "    outgoing_edge_nums[node] = len(succ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adjacency_backward_array, incoming_edge_nums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adjacency_forward_array, outgoing_edge_nums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## all inputs\n",
        "num_heads = 4\n",
        "head_dim = 32\n",
        "qk_head_dim = 32\n",
        "v_head_dim = 32\n",
        "\n",
        "adjacency_backward_array, incoming_edge_nums\n",
        "adjacency_forward_array, outgoing_edge_nums\n",
        "\n",
        "inp = torch.zeros([num_heads, num_nodes, head_dim])\n",
        "query = torch.zeros([num_heads, num_nodes, qk_head_dim], requires_grad=True)\n",
        "key = torch.zeros([num_heads, num_nodes, qk_head_dim], requires_grad=True)\n",
        "value = torch.zeros([num_heads, num_nodes, v_head_dim], requires_grad=True)\n",
        "\n",
        "source = torch.zeros([num_heads, num_nodes, max_edges], requires_grad=True)\n",
        "transition = torch.zeros([num_heads, num_nodes, max_edges, max_edges], requires_grad=True)\n",
        "transition_mask = torch.ones([num_heads, num_nodes, max_edges, max_edges], requires_grad=True)\n",
        "mark = torch.zeros([num_heads, num_nodes, max_edges], requires_grad=True)\n",
        "direct = torch.zeros([num_heads, num_nodes], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Description of a pLSTM-Graph layer\n",
        "Vector-Valued inputs at nodes are split into head vectors\n",
        "Source, Transition and Mark have different scaling \"angle\" depending on head and number of predecessors / successors\n",
        "Source doesn't have to scale, Mark doesn't have to scale\n",
        "Transition should be limited to one in row / column\n",
        "Transition should distribute differently for different heads in bias -> bias term is not constant but adaptive to number of pred/succ\n",
        "Example: \n",
        "4 heads\n",
        "node: 2 pred, 2 succ -> \"orientation bias\" according to succ: 1 angle \n",
        "node: 2 pred, 4 succ -> \"orientation bias\" according to succ: 3 angles\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Approach: No parallelization for now -> sequential processing. \n",
        "Problem: All edges need a C state potentially. Need to store the state as well for backprop. C-States have size: qkdim x vdim.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Naive implementation, C state for every edge -> potentially recompute for backward to save memory.\n",
        "# 1. source terms given\n",
        "# 2. compute edge states for every edge in DAG order of linegraph, use transitions\n",
        "# 3. compute outputs via marks\n",
        "\n",
        "cell_states = torch.zeros([num_heads, num_edges, qk_head_dim, v_head_dim])\n",
        "outputs = torch.zeros([num_heads, num_nodes, v_head_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lDAG = nx.line_graph(DAG)\n",
        "lDAG_edges = list(nx.topological_sort(lDAG))\n",
        "\n",
        "idx_map = {edge: idx for idx, edge in enumerate(lDAG_edges)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edge_out_map = {}\n",
        "edge_in_map = {}\n",
        "for idx_edge, edge in enumerate(lDAG_edges):\n",
        "    in_node = edge[0]\n",
        "    out_node = edge[1]\n",
        "    for pred_edge in lDAG.predecessors(edge):\n",
        "        cell_states[:, idx_edge] += (\n",
        "            transition[:, in_node, adjacency_forward_edgemap[pred_edge], adjacency_backward_edgemap[edge], None, None]\n",
        "            * cell_states[:, idx_map[pred_edge]]\n",
        "        )\n",
        "    cell_states[:, idx_edge] += source[:, in_node, adjacency_backward_edgemap[edge], None, None] * torch.einsum(\n",
        "        \"ha,hb->hab\", key[:, in_node], value[:, in_node]\n",
        "    )\n",
        "    outputs[:, out_node] += mark[:, out_node, adjacency_forward_edgemap[edge], None] * torch.einsum(\n",
        "        \"ha,hab->hb\", query[:, out_node], cell_states[:, idx_edge]\n",
        "    )\n",
        "\n",
        "outputs += direct[:, :, None] * torch.sum(key * query, dim=-1, keepdim=True) * value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plstm_graph(\n",
        "    query,\n",
        "    key,\n",
        "    value,\n",
        "    source,\n",
        "    transition,\n",
        "    mark,\n",
        "    direct,\n",
        "    adjancency_forward_edgemap,\n",
        "    adjacency_backward_edgemap,\n",
        "    lDAG,\n",
        "    lDAG_sorted,\n",
        "    recompute_cell_states: bool = True,\n",
        "):\n",
        "    class pLSTMGraph(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(\n",
        "            ctx,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            source,\n",
        "            transition,\n",
        "            mark,\n",
        "            direct,\n",
        "            adjancency_forward_edgemap,\n",
        "            adjacency_backward_edgemap,\n",
        "            lDAG,\n",
        "            lDAG_sorted,\n",
        "        ):\n",
        "            num_heads, num_nodes, qk_head_dim, v_head_dim = *query.shape, value.shape[-1]\n",
        "            cell_states = torch.zeros([num_heads, num_edges, qk_head_dim, v_head_dim])\n",
        "            outputs = torch.zeros([num_heads, num_nodes, v_head_dim])\n",
        "\n",
        "            for idx_edge, edge in enumerate(lDAG_sorted):\n",
        "                in_node = edge[0]\n",
        "                out_node = edge[1]\n",
        "                for pred_edge in lDAG.predecessors(edge):\n",
        "                    cell_states[:, idx_edge] += (\n",
        "                        transition[\n",
        "                            :,\n",
        "                            in_node,\n",
        "                            adjacency_forward_edgemap[pred_edge],\n",
        "                            adjacency_backward_edgemap[edge],\n",
        "                            None,\n",
        "                            None,\n",
        "                        ]\n",
        "                        * cell_states[:, idx_map[pred_edge]]\n",
        "                    )\n",
        "                cell_states[:, idx_edge] += source[\n",
        "                    :, in_node, adjacency_backward_edgemap[edge], None, None\n",
        "                ] * torch.einsum(\"hk,hv->hkv\", key[:, in_node], value[:, in_node])\n",
        "                outputs[:, out_node] += mark[:, out_node, adjacency_forward_edgemap[edge], None] * torch.einsum(\n",
        "                    \"hk,hkv->hv\", query[:, out_node], cell_states[:, idx_edge]\n",
        "                )\n",
        "\n",
        "            outputs += direct[:, :, None] * torch.sum(key * query, dim=-1, keepdim=True) * value\n",
        "\n",
        "            ctx.save_for_backward(\n",
        "                query, key, value, source, transition, mark, direct, cell_states if not recompute_cell_states else None\n",
        "            )\n",
        "            ctx.lDAG = lDAG\n",
        "            ctx.lDAG_sorted = lDAG_sorted\n",
        "            ctx.adjacency_forward_edgemap = adjacency_forward_edgemap\n",
        "            ctx.adjacency_backward_edgemap = adjacency_backward_edgemap\n",
        "\n",
        "            return outputs\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(\n",
        "            ctx, doutputs\n",
        "        ) -> tuple[\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "        ]:\n",
        "            query, key, value, source, transition, mark, direct, cell_states = ctx.saved_tensors\n",
        "            num_heads, _, qk_head_dim, v_head_dim = *query.shape, value.shape[-1]\n",
        "\n",
        "            dquery = torch.zeros_like(query)\n",
        "            dkey = torch.zeros_like(key)\n",
        "            dvalue = torch.zeros_like(value)\n",
        "            dsource = torch.zeros_like(source)\n",
        "            dtransition = torch.zeros_like(transition)\n",
        "            dmark = torch.zeros_like(mark)\n",
        "            ddirect = torch.zeros_like(direct)\n",
        "\n",
        "            if not cell_states:\n",
        "                cell_states = torch.zeros([num_heads, num_edges, qk_head_dim, v_head_dim])\n",
        "                for idx_edge, edge in enumerate(lDAG_sorted):\n",
        "                    in_node = edge[0]\n",
        "                    out_node = edge[1]\n",
        "                    for pred_edge in lDAG.predecessors(edge):\n",
        "                        cell_states[:, idx_edge] += (\n",
        "                            transition[\n",
        "                                :,\n",
        "                                in_node,\n",
        "                                adjacency_forward_edgemap[pred_edge],\n",
        "                                adjacency_backward_edgemap[edge],\n",
        "                                None,\n",
        "                                None,\n",
        "                            ]\n",
        "                            * cell_states[:, idx_map[pred_edge]]\n",
        "                        )\n",
        "                    cell_states[:, idx_edge] += source[\n",
        "                        :, in_node, adjacency_backward_edgemap[edge], None, None\n",
        "                    ] * torch.einsum(\"ha,hb->hab\", key[:, in_node], value[:, in_node])\n",
        "\n",
        "            dcell_states = torch.zeros_like(cell_states)\n",
        "\n",
        "            for revidx_edge, edge in enumerate(reversed(ctx.lDAG_sorted)):\n",
        "                idx_edge = num_edges - revidx_edge - 1\n",
        "                in_node = edge[0]\n",
        "                out_node = edge[1]\n",
        "                for succ_edge in lDAG.successors(edge):\n",
        "                    dcell_states[:, idx_edge] += (\n",
        "                        transition[\n",
        "                            :,\n",
        "                            out_node,\n",
        "                            adjacency_forward_edgemap[edge],\n",
        "                            adjacency_backward_edgemap[succ_edge],\n",
        "                            None,\n",
        "                            None,\n",
        "                        ]\n",
        "                        * dcell_states[:, idx_map[succ_edge]]\n",
        "                    )\n",
        "                    dtransition[\n",
        "                        :, out_node, adjacency_forward_edgemap[edge], adjacency_backward_edgemap[succ_edge]\n",
        "                    ] += torch.einsum(\"hkv,hkv->h\", cell_states[:, idx_edge], dcell_states[:, idx_map[succ_edge]])\n",
        "                dcell_states[:, idx_edge] += mark[\n",
        "                    :, out_node, adjacency_forward_edgemap[edge], None, None\n",
        "                ] * torch.einsum(\"hk,hv->hkv\", query[:, out_node], doutputs[:, out_node])\n",
        "\n",
        "                dquery[:, out_node] += mark[:, out_node, adjacency_forward_edgemap[edge], None] * torch.einsum(\n",
        "                    \"hkv,hv->hk\", cell_states[:, idx_edge], doutputs[:, out_node]\n",
        "                )\n",
        "                dmark[:, out_node, adjacency_forward_edgemap[edge]] += torch.einsum(\n",
        "                    \"hk,hkv,hv->h\", query[:, out_node], cell_states[:, idx_edge], doutputs[:, out_node]\n",
        "                )\n",
        "\n",
        "                dkey[:, in_node] += source[:, in_node, adjacency_backward_edgemap[edge], None] * torch.einsum(\n",
        "                    \"hkv,hv->hk\", dcell_states[:, idx_edge], value[:, in_node]\n",
        "                )\n",
        "                dvalue[:, in_node] += source[:, in_node, adjacency_backward_edgemap[edge], None] * torch.einsum(\n",
        "                    \"hkv,hk->hv\", dcell_states[:, idx_edge], key[:, in_node]\n",
        "                )\n",
        "                dsource[:, in_node, adjacency_backward_edgemap[edge]] += torch.einsum(\n",
        "                    \"hkv,hk,hv\", dcell_states[:, idx_edge], key[:, in_node], value[:, in_node]\n",
        "                )\n",
        "\n",
        "            dquery += torch.einsum(\"hn,hnk,hnv,hnv->hnk\", direct, key, value, doutputs)\n",
        "            dkey += torch.einsum(\"hn,hnk,hnv,hnv->hnk\", direct, query, value, doutputs)\n",
        "            dvalue += torch.einsum(\"hn,hnk,hnk,hnv->hnk\", direct, query, key, doutputs)\n",
        "            ddirect += torch.einsum(\"hnk,hnk,hnv,hnv->hn\", query, key, value, doutputs)\n",
        "\n",
        "            return dquery, dkey, dvalue, dsource, dtransition, dmark, ddirect, None, None, None, None\n",
        "\n",
        "    return pLSTMGraph.apply(\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        source,\n",
        "        transition,\n",
        "        mark,\n",
        "        direct,\n",
        "        adjancency_forward_edgemap,\n",
        "        adjacency_backward_edgemap,\n",
        "        lDAG,\n",
        "        lDAG_sorted,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = plstm_graph(\n",
        "    query,\n",
        "    key,\n",
        "    value,\n",
        "    source,\n",
        "    transition,\n",
        "    mark,\n",
        "    direct,\n",
        "    adjacency_forward_edgemap,\n",
        "    adjacency_backward_edgemap,\n",
        "    lDAG,\n",
        "    lDAG_edges,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = plstm_graph(\n",
        "    query,\n",
        "    key,\n",
        "    value,\n",
        "    source,\n",
        "    transition,\n",
        "    mark,\n",
        "    direct,\n",
        "    adjacency_forward_edgemap,\n",
        "    adjacency_backward_edgemap,\n",
        "    lDAG,\n",
        "    lDAG_edges,\n",
        ")\n",
        "loss = torch.sum(out)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### plstm graph transition normalization P mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Given a list of transitions t: [H, N, E, E], with H heads, N nodes, and E max edges per node, they have to be normalized,\n",
        "such that torch.sum(torch.abs(t), dim=3) <= 1. \n",
        "Also they should be normalized such that the transitions can be between minus one and one, in all cases. \n",
        "Given values: t1, t2, t3, t4... , max edges E, real edges e, arbitrary.\n",
        "Out values: n1, n2, n3, n4, ..., s.t. Sum |n1| + |n2| +... <= 1\n",
        "\n",
        "Use L1 norm right away with:\n",
        "ni = ti / (1 + alpha * l1)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of this.\n",
        "\n",
        "given: \n",
        "vector [H, N, E, E]\n",
        "actual incoming_edge_nums: [N]\n",
        "\n",
        "Set non-existant edges to zero:\n",
        "edge_mask: [N, E] s.t. em = (incoming_edge_nums + 0.5 - arange(E)) > 0\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test pLSTMGraphLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plstm.torch.plstm_graph_layer import pLSTMGraphLayerConfig, pLSTMGraphLayer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = pLSTMGraphLayerConfig(input_dim=64, num_heads=4, max_edges=8, mode=\"P\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = pLSTMGraphLayer(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inp = torch.randn((G.number_of_nodes(), cfg.input_dim))\n",
        "\n",
        "out = layer(inp, graph=G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test pLSTMGraphEdgeLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plstm.torch.plstm_graph_layer import pLSTMGraphEdgeLayerConfig, pLSTMGraphEdgeLayer, PreparedGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = PreparedGraph.create(G, mode=\"P\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(G.edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plstm.graph import dagify\n",
        "\n",
        "dag2 = dagify(G)\n",
        "print(len(dag2.edges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(g.dag.edges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = pLSTMGraphEdgeLayerConfig(\n",
        "    input_dim=64,\n",
        "    num_heads=4,\n",
        "    edge_input_dim=32,\n",
        "    max_edges=100,  # not actually used\n",
        "    mode=\"P\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pge_layer = pLSTMGraphEdgeLayer(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inp = torch.randn((G.number_of_nodes(), cfg.input_dim))\n",
        "edge_inp = torch.randn((G.number_of_edges(), cfg.edge_input_dim))\n",
        "\n",
        "out = pge_layer(inp, edge_inp, graph=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.sum().backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Important TODO:\n",
        "\"\"\"\n",
        "- pLSTMGraphEdgeLayer: P Mode normalization!!! -> works?\n",
        "- check if indexing is aligned -> fixed?\n",
        "- transition biases!!!\n",
        "- thorough testing!\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pLSTM Graph Block Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plstm.config.graph_block import pLSTMGraphBlockConfig, pLSTMGraphEdgeBlockConfig\n",
        "from plstm.torch.interfaces import ResidualModule\n",
        "\n",
        "cfg = pLSTMGraphBlockConfig(input_dim=192, num_heads=12, block_mode=\"DP\", block_type=\"post_up\", max_edges=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from compoconf import Registry\n",
        "\n",
        "Registry._registries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_block = cfg.instantiate(ResidualModule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inp = torch.randn((G.number_of_nodes(), cfg.input_dim))\n",
        "# edge_inp = torch.randn((G.number_of_edges(), cfg.edge_input_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_block(inp, graph=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = pLSTMGraphEdgeBlockConfig(input_dim=192, num_heads=12, block_mode=\"DP\", block_type=\"post_up\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_block = cfg.instantiate(ResidualModule)\n",
        "inp = torch.randn((G.number_of_nodes(), cfg.input_dim))\n",
        "edge_inp = torch.randn((G.number_of_edges(), cfg.input_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_block(inp, edge_features=edge_inp, graph=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "plstm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
